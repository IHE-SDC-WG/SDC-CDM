from dataclasses import dataclass
from enum import Enum, auto
from pathlib import Path
import re
from typing import Dict, List, Optional

ORIGINAL_SCHEMA_NAME = "@cdmDatabaseSchema"


class DatabaseType(Enum):
    POSTGRESQL = auto()
    SQLITE = auto()
    SQLSERVER = auto()


@dataclass
class DatabaseConfig:
    schema_name: str
    file_prefix_map: Dict[str, str]

    @classmethod
    def get_config(cls, db_type: DatabaseType) -> "DatabaseConfig":
        base_prefix_map = {
            "ddl": "1_",
            "primary_keys": "2_",
            "constraints": "3_",
            "indices": "4_",
        }

        configs = {
            DatabaseType.POSTGRESQL: cls(
                schema_name="public", file_prefix_map=base_prefix_map
            ),
            DatabaseType.SQLITE: cls(
                schema_name="main", file_prefix_map=base_prefix_map
            ),
            DatabaseType.SQLSERVER: cls(
                schema_name="dbo", file_prefix_map=base_prefix_map
            ),
        }
        return configs[db_type]


def extract_primary_keys(content: str) -> Dict[str, str]:
    """Extract table and column names from primary key constraints"""
    pk_info = {}
    for line in content.split("\n"):
        if "ADD CONSTRAINT" in line and "PRIMARY KEY" in line:
            match = re.search(
                rf"ALTER TABLE {ORIGINAL_SCHEMA_NAME}\.(\w+).*PRIMARY KEY \((\w+)\)",
                line,
            )
            if match:
                table_name = match.group(1)
                pk_column = match.group(2)
                pk_info[table_name.lower()] = pk_column.lower()
    return pk_info


def extract_foreign_keys(content: str) -> Dict[str, List[tuple]]:
    """Extract foreign key relationships from constraints"""
    fk_info = {}
    for line in content.split("\n"):
        if "ADD CONSTRAINT" in line and "FOREIGN KEY" in line:
            match = re.search(
                rf"ALTER TABLE {ORIGINAL_SCHEMA_NAME}\.(\w+).*FOREIGN KEY \((\w+)\) REFERENCES {ORIGINAL_SCHEMA_NAME}\.(\w+) \((\w+)\)",
                line,
            )
            if match:
                table_name = match.group(1).lower()
                fk_column = match.group(2).lower()
                ref_table = match.group(3).lower()
                ref_column = match.group(4).lower()

                if table_name not in fk_info:
                    fk_info[table_name] = []
                fk_info[table_name].append((fk_column, ref_table, ref_column))
    return fk_info


def modify_sqlite_column_definition(
    create_table_stmt: str,
    pk_info: Optional[Dict[str, str]] = None,
    fk_info: Optional[Dict[str, List[tuple]]] = None,
) -> str:
    """Modify column definitions for SQLite to include PRIMARY KEY and FOREIGN KEY"""
    table_name_match = re.search(
        rf"CREATE TABLE {ORIGINAL_SCHEMA_NAME}\.(\w+)", create_table_stmt, re.IGNORECASE
    )
    if not table_name_match:
        return create_table_stmt

    table_name = table_name_match.group(1).lower()
    modified_stmt = create_table_stmt

    # Handle primary key
    if pk_info and table_name in pk_info:
        pk_column = pk_info[table_name]
        pattern = rf"(\s+{pk_column}\s+integer\s+NOT NULL)(,|\s*\))"
        replacement = r"\1 PRIMARY KEY AUTOINCREMENT\2"
        modified_stmt = re.sub(pattern, replacement, modified_stmt, flags=re.IGNORECASE)

    # Handle foreign keys
    if fk_info and table_name in fk_info:
        for fk_column, ref_table, ref_column in fk_info[table_name]:
            pattern = rf"(\s+{fk_column}\s+integer\s+NOT NULL)(,|\s*\))"
            replacement = rf"\1 REFERENCES {ref_table}({ref_column})\2"
            modified_stmt = re.sub(
                pattern, replacement, modified_stmt, flags=re.IGNORECASE
            )

    return modified_stmt


def process_sqlite_content(
    content: str,
    comment_lines: bool = False,
    pk_info: Optional[Dict[str, str]] = None,
    fk_info: Optional[Dict[str, List[tuple]]] = None,
) -> str:
    """Process content specifically for SQLite database"""
    schema_name = DatabaseConfig.get_config(DatabaseType.SQLITE).schema_name

    if comment_lines:
        content = (
            "--This file was originally generated by the OHDSI/CommonDataModel project.\n"
            "--All contents have been commented out as Sqlite cannot execute 'ALTER TABLE' statements with 'ADD CONSTRAINT'.\n"
            "--Each constraint has been applied to the 1_OMOPCDM_sqlite_*_ddl.sql file.\n"
            "--------------------------------------------------------------------------------\n\n"
            + "\n".join(
                f"-- {line}" if line.strip() else line for line in content.split("\n")
            )
        )
    elif pk_info is not None or fk_info is not None:
        content = (
            "-- Enable foreign key constraints\nPRAGMA foreign_keys = ON;\n\n"
            "BEGIN TRANSACTION;\n\n" + content + "\nCOMMIT;\n"
        )
        statements = re.split(
            r"(CREATE TABLE.*?;)", content, flags=re.DOTALL | re.IGNORECASE
        )
        processed_statements = []
        for stmt in statements:
            if stmt.strip().upper().startswith("CREATE TABLE"):
                stmt = modify_sqlite_column_definition(stmt, pk_info, fk_info)
            processed_statements.append(stmt)
        content = "".join(processed_statements)

    # Replace schema name
    content = content.replace("@cdmDatabaseSchema", schema_name)
    return content


def process_postgresql_content(content: str) -> str:
    """Process content specifically for PostgreSQL database"""
    schema_name = DatabaseConfig.get_config(DatabaseType.POSTGRESQL).schema_name
    return content.replace("@cdmDatabaseSchema", schema_name)


def process_sqlserver_content(content: str) -> str:
    """Process content specifically for SQL Server database"""
    schema_name = DatabaseConfig.get_config(DatabaseType.SQLSERVER).schema_name
    return content.replace("@cdmDatabaseSchema", schema_name)


def process_ddl_files(
    input_files: Dict[str, str], db_type: DatabaseType
) -> Dict[str, str]:
    """
    Process DDL files for a specific database type
    Returns a dictionary of processed file contents
    """
    config = DatabaseConfig.get_config(db_type)
    processed_files = {}

    # Read all input files
    file_contents = {
        key: Path(filepath).read_text(encoding="utf-8")
        for key, filepath in input_files.items()
    }

    if db_type == DatabaseType.SQLITE:
        # Extract key information for SQLite
        pk_info = extract_primary_keys(file_contents["primary_keys"])
        fk_info = extract_foreign_keys(file_contents["constraints"])

        # Process each file according to SQLite requirements
        processed_files.update(
            {
                f"{config.file_prefix_map['ddl']}{Path(input_files['ddl']).name}": process_sqlite_content(
                    file_contents["ddl"], pk_info=pk_info, fk_info=fk_info
                ),
                f"{config.file_prefix_map['primary_keys']}{Path(input_files['primary_keys']).name}": process_sqlite_content(
                    file_contents["primary_keys"], comment_lines=True
                ),
                f"{config.file_prefix_map['constraints']}{Path(input_files['constraints']).name}": process_sqlite_content(
                    file_contents["constraints"], comment_lines=True
                ),
                f"{config.file_prefix_map['indices']}{Path(input_files['indices']).name}": process_sqlite_content(
                    file_contents["indices"]
                ),
            }
        )

    elif db_type == DatabaseType.POSTGRESQL:
        # Process each file according to PostgreSQL requirements
        processed_files.update(
            {
                f"{config.file_prefix_map[key]}{Path(filepath).name}": process_postgresql_content(
                    file_contents[key]
                )
                for key, filepath in input_files.items()
            }
        )

    elif db_type == DatabaseType.SQLSERVER:
        # Process each file according to SQL Server requirements
        processed_files.update(
            {
                f"{config.file_prefix_map[key]}{Path(filepath).name}": process_sqlserver_content(
                    file_contents[key]
                )
                for key, filepath in input_files.items()
            }
        )

    return processed_files
